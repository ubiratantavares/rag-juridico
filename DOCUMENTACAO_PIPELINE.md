# Documenta√ß√£o T√©cnica: Pipeline RAG Jur√≠dico

Este documento detalha o fluxo completo do projeto, desde o carregamento dos documentos legais at√© a gera√ß√£o de respostas especializadas utilizando Intelig√™ncia Artificial (RAG - *Retrieval-Augmented Generation*).

## üèóÔ∏è Arquitetura Geral

O projeto utiliza uma arquitetura **Model-View-Controller (MVC)** adaptada para fluxos de dados de IA:

- **Model**: Gerenciado por `src/ingestao.py` (dados) e `src/rag.py` (IA).
- **View**: Implementada em `app.py` como a interface de terminal.
- **Controller**: Orquestra o fluxo entre o carregamento e a intera√ß√£o no `app.py`.

## üöÄ Fluxo Passo a Passo

### 1. Configura√ß√£o e Ambiente

O pipeline come√ßa garantindo que as credenciais e depend√™ncias estejam corretas.

- **Arquivo**: `.env`, `requirements.txt`.
- **A√ß√£o**: O script `tests/test_setup.py` valida se a `GOOGLE_API_KEY` existe e se as bibliotecas como `langchain-google-genai` e `chromadb` est√£o prontas.

### 2. Ingest√£o de Documentos (Ingestion)

O objetivo √© transformar arquivos PDF brutos em objetos de dados que a IA compreenda.

- **M√≥dulo**: `src/ingestao.py` (`LegalPDFLoader`).
- **Passo**:
  - O sistema l√™ os arquivos em `./dados/` (CDC e LGPD).
  - **Metadados**: Cada p√°gina recebe uma tag (`fonte: cdc` ou `fonte: lgpd`). Isso √© crucial para que o modelo cite a fonte correta na resposta final.

### 3. Processamento de Texto (Chunking)

Documentos jur√≠dicos s√£o longos demais para o contexto do modelo. Precisamos "fati√°-los".

- **M√≥dulo**: `src/ingestao.py` (`DocumentProcessor`).
- **Estrat√©gia**: Utilizamos a `RecursiveCharacterTextSplitter` com:
  - **Chunk Size**: 1500 caracteres (tamanho ideal para manter o par√°grafo).
  - **Overlap**: 300 caracteres (evita que uma frase seja cortada ao meio entre dois peda√ßos de papel).

### 4. Indexa√ß√£o Vetorial (Vector Store)

Transformamos os textos em n√∫meros (vetores) para permitir buscas por significado.

- **M√≥dulo**: `src/rag.py` (`VectorDatabaseManager`).
- **Embeddings**: Utilizamos o modelo `gemini-embedding-001`.
- **Banco**: O **ChromaDB** armazena esses vetores localmente na pasta `chroma_db/`. Isso permite que o sistema funcione sem precisar reprocessar os PDFs toda vez.

### 5. Recupera√ß√£o Sem√¢ntica (Retrieval)

Quando voc√™ faz uma pergunta, o sistema n√£o busca por palavras exatas, mas por conceitos.

- **M√≥dulo**: `src/rag.py` (`VectorDatabaseManager.search`).
- **A√ß√£o**: O sistema converte sua pergunta em um vetor e busca os fragmentos mais similares no ChromaDB.

### 6. Reranking com LLM

Para garantir a m√°xima precis√£o, inclu√≠mos uma etapa de refinamento.

- **M√≥dulo**: `src/rag.py` (`RAGChainManager.rerank`).
- **Fluxo**:
    1. O sistema recupera 10 fragmentos candidatos (Busca Vetorial).
    2. Envia esses 10 peda√ßos para o **Gemini 2.0 Flash** em lote (*Batch*).
    3. O modelo avalia a relev√¢ncia de cada um e devolve os **4 IDs mais importantes**.
    4. Esta t√©cnica garante que a resposta final use apenas o contexto mais pertinente.

### 7. Gera√ß√£o de Resposta (Generation)

A fase final onde a resposta √© redigida.

- **M√≥dulo**: `src/rag.py` (`RAGChainManager.ask`).
- **Prompt**:
  - Instru√≠mos o modelo a ser um "Assistente Jur√≠dico".
  - Ele √© proibido de usar conhecimento externo: **"Responda APENAS com o contexto fornecido"**.
  - Ele deve citar obrigatoriamente se a informa√ß√£o veio do CDC ou da LGPD.

### 8. Valida√ß√£o e Testes

Garantimos que cada engrenagem do pipeline continue funcionando.

- **Pasta**: `tests/`.
- **Ferramenta**: `pytest`.
- **Testes**: Cobrem desde o setup b√°sico at√© a l√≥gica complexa de reranking e a precis√£o da resposta do RAG.

## üõ†Ô∏è Como o Pipeline √© Acionado

O usu√°rio interage via `app.py`:

1. **Op√ß√£o [1]**: Roda os passos 1 ao 4 (Limpa o banco e recria a partir dos PDFs).
2. **Op√ß√£o [2]**: Roda os passos 5 ao 7 (Inicia o chat interativo com busca, rerank e resposta).

*Documenta√ß√£o gerada para a Sprint 1 do projeto RAG Jur√≠dico.*
